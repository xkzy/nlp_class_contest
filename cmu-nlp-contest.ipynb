{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('/kaggle/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfor dirname, _, filenames in os.walk('/kaggle/temp'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-02T02:50:16.893251Z","iopub.execute_input":"2023-10-02T02:50:16.893656Z","iopub.status.idle":"2023-10-02T02:50:16.926914Z","shell.execute_reply.started":"2023-10-02T02:50:16.893590Z","shell.execute_reply":"2023-10-02T02:50:16.925952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install deepcut -q -q -q --exists-action i\n!pip install pythainlp['full'] -q -q -q --exists-action i","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:50:26.632443Z","iopub.execute_input":"2023-10-02T02:50:26.632818Z","iopub.status.idle":"2023-10-02T02:50:45.642326Z","shell.execute_reply.started":"2023-10-02T02:50:26.632788Z","shell.execute_reply":"2023-10-02T02:50:45.640925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import subprocess\nfrom keras.callbacks import ModelCheckpoint\nimport tensorflow as tf\nimport csv\nimport numpy as np\nimport deepcut\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom tensorflow.keras.utils import to_categorical, set_random_seed\nimport matplotlib.pyplot as plt\nfrom random import shuffle\nimport pathlib\nimport random\nimport string\nimport re\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization\nimport pythainlp\nimport pickle\n\nset_random_seed(99)","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:50:58.563102Z","iopub.execute_input":"2023-10-02T02:50:58.563493Z","iopub.status.idle":"2023-10-02T02:50:58.640538Z","shell.execute_reply.started":"2023-10-02T02:50:58.563463Z","shell.execute_reply":"2023-10-02T02:50:58.639579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"try:\n    subprocess.check_output('nvidia-smi')\n    print('Nvidia GPU detected!')\nexcept Exception: # this command not being found can raise quite a few different errors depending on the configuration\n    print('No Nvidia GPU in system!')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:51:00.858616Z","iopub.execute_input":"2023-10-02T02:51:00.859505Z","iopub.status.idle":"2023-10-02T02:51:00.985230Z","shell.execute_reply.started":"2023-10-02T02:51:00.859474Z","shell.execute_reply":"2023-10-02T02:51:00.984201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tf.__version__)\ntf.test.is_gpu_available()\nprint(tf.config.list_physical_devices())","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:51:04.048256Z","iopub.execute_input":"2023-10-02T02:51:04.048652Z","iopub.status.idle":"2023-10-02T02:51:04.087454Z","shell.execute_reply.started":"2023-10-02T02:51:04.048590Z","shell.execute_reply":"2023-10-02T02:51:04.086197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentence_loader(input_file):\n    out=[]\n    for i in list(input_file):\n        s = i.split('::')[1].strip()\n        s = ' '.join(pythainlp.word_tokenize(s,engine = 'deepcut'))\n        out.append(s)\n    return out\n\ndef answer_loader(ans_file):\n    out=[]\n    for i in ans_file:\n        s = '[start] '+i.split('::')[1].strip().replace(\",\", \" \")+' [end]'\n        out.append(s)\n    return out\n\ndef pair_gen(s,a):\n    return list(zip(s,a))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:51:06.741015Z","iopub.execute_input":"2023-10-02T02:51:06.742132Z","iopub.status.idle":"2023-10-02T02:51:06.749694Z","shell.execute_reply.started":"2023-10-02T02:51:06.742087Z","shell.execute_reply":"2023-10-02T02:51:06.748626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_file = open('/kaggle/input/typeof/input.txt', 'r',encoding = 'utf-8-sig')\nans_file = open('/kaggle/input/typeof/ans.txt', 'r',encoding = 'utf-8-sig')","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:51:11.069118Z","iopub.execute_input":"2023-10-02T02:51:11.069468Z","iopub.status.idle":"2023-10-02T02:51:11.076881Z","shell.execute_reply.started":"2023-10-02T02:51:11.069439Z","shell.execute_reply":"2023-10-02T02:51:11.075683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_data = sentence_loader(input_file)\nans_data = answer_loader(ans_file)\ntext_pairs = []\n# try:\nwith open('text_pairs.pkl', 'rb') as f:\n    text_pairs = pickle.load(f)\n    print(text_pairs)\n# except:\n#     pass\n\n# if len(text_pairs) <= 0:\n#     text_pairs = pair_gen(input_data,ans_data)\n#     with open('text_pairs.pkl', 'wb') as f:\n#         pickle.dump(text_pairs, f)\n\nprint(len(text_pairs))\nfor _ in range(5):\n    print(random.choice(text_pairs))","metadata":{"execution":{"iopub.status.busy":"2023-10-02T02:53:37.981386Z","iopub.execute_input":"2023-10-02T02:53:37.981775Z","iopub.status.idle":"2023-10-02T02:53:38.020307Z","shell.execute_reply.started":"2023-10-02T02:53:37.981743Z","shell.execute_reply":"2023-10-02T02:53:38.019020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.shuffle(text_pairs)\nnum_val_samples = int(0.15 * len(text_pairs))\nnum_train_samples = len(text_pairs) - 2 * num_val_samples\ntrain_pairs = text_pairs[:num_train_samples]\nval_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\ntest_pairs = text_pairs[num_train_samples + num_val_samples :]\n\nprint(f\"{len(text_pairs)} total pairs\")\nprint(f\"{len(train_pairs)} training pairs\")\nprint(f\"{len(val_pairs)} validation pairs\")\nprint(f\"{len(test_pairs)} test pairs\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from posixpath import split\nstrip_chars = string.punctuation + \"¿\"\nstrip_chars = strip_chars.replace(\"[\", \"\")\nstrip_chars = strip_chars.replace(\"]\", \"\")\n\nvocab_size = 15000\nsequence_length = 32\nbatch_size = 32\n\n\ndef custom_standardization(input_string):\n    lowercase = tf.strings.lower(input_string)\n    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n\n\ninput_vectorization = TextVectorization(\n    max_tokens=vocab_size,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n    split='whitespace',\n)\nans_vectorization = TextVectorization(\n    max_tokens=10,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length + 1,\n    standardize=custom_standardization,\n    split='whitespace',\n)\ntrain_input_texts = [pair[0] for pair in train_pairs]\ntrain_ans_texts = [pair[1] for pair in train_pairs]\ninput_vectorization.adapt(train_input_texts)\nans_vectorization.adapt(train_ans_texts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def format_dataset(input, ans):\n    input = input_vectorization(input)\n    ans = ans_vectorization(ans)\n    return (\n        {\n            \"encoder_inputs\": input,\n            \"decoder_inputs\": ans[:, :-1],\n        },\n        ans[:, 1:],\n    )\n\n\ndef make_dataset(pairs):\n    input_texts, ans_texts = zip(*pairs)\n    input_texts = list(input_texts)\n    ans_texts = list(ans_texts)\n    dataset = tf.data.Dataset.from_tensor_slices((input_texts, ans_texts))\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.map(format_dataset)\n    return dataset.shuffle(2048).prefetch(16).cache()\n\n\ntrain_ds = make_dataset(train_pairs)\nval_ds = make_dataset(val_pairs)\ntest_ds = make_dataset(test_pairs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for inputs, targets in train_ds.take(1):\n    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n    print(f\"targets.shape: {targets.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class TransformerEncoder(layers.Layer):\n    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.dense_dim = dense_dim\n        self.num_heads = num_heads\n        self.attention = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(dense_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.supports_masking = True\n\n    def call(self, inputs, mask=None):\n        attention_output = self.attention(query=inputs, value=inputs, key=inputs)\n        proj_input = self.layernorm_1(inputs + attention_output)\n        proj_output = self.dense_proj(proj_input)\n        return self.layernorm_2(proj_input + proj_output)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"embed_dim\": self.embed_dim,\n                \"dense_dim\": self.dense_dim,\n                \"num_heads\": self.num_heads,\n            }\n        )\n        return config\n\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.token_embeddings = layers.Embedding(\n            input_dim=vocab_size, output_dim=embed_dim\n        )\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=embed_dim\n        )\n        self.sequence_length = sequence_length\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n\n    def call(self, inputs):\n        length = tf.shape(inputs)[-1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_tokens = self.token_embeddings(inputs)\n        embedded_positions = self.position_embeddings(positions)\n        return embedded_tokens + embedded_positions\n\n    def compute_mask(self, inputs, mask=None):\n        return tf.math.not_equal(inputs, 0)\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"sequence_length\": self.sequence_length,\n                \"vocab_size\": self.vocab_size,\n                \"embed_dim\": self.embed_dim,\n            }\n        )\n        return config\n\n\nclass TransformerDecoder(layers.Layer):\n    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n        super().__init__(**kwargs)\n        self.embed_dim = embed_dim\n        self.latent_dim = latent_dim\n        self.num_heads = num_heads\n        self.attention_1 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.attention_2 = layers.MultiHeadAttention(\n            num_heads=num_heads, key_dim=embed_dim\n        )\n        self.dense_proj = keras.Sequential(\n            [\n                layers.Dense(latent_dim, activation=\"relu\"),\n                layers.Dense(embed_dim),\n            ]\n        )\n        self.layernorm_1 = layers.LayerNormalization()\n        self.layernorm_2 = layers.LayerNormalization()\n        self.layernorm_3 = layers.LayerNormalization()\n        self.add = layers.Add()  # instead of `+` to preserve mask\n        self.supports_masking = True\n\n    def call(self, inputs, encoder_outputs, mask=None):\n        attention_output_1 = self.attention_1(\n            query=inputs, value=inputs, key=inputs, use_causal_mask=True\n        )\n        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n\n        attention_output_2 = self.attention_2(\n            query=out_1,\n            value=encoder_outputs,\n            key=encoder_outputs,\n        )\n        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n\n        proj_output = self.dense_proj(out_2)\n        return self.layernorm_3(self.add([out_2, proj_output]))\n\n    def get_config(self):\n        config = super().get_config()\n        config.update(\n            {\n                \"embed_dim\": self.embed_dim,\n                \"latent_dim\": self.latent_dim,\n                \"num_heads\": self.num_heads,\n            }\n        )\n        return config","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 300\nlatent_dim = 2048\nnum_heads = 16\nnum_en_transformer_blocks = 1\nnum_de_transformer_blocks = 1\n\n\nencoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\nfor _ in range(num_en_transformer_blocks):\n    x = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\nencoder_outputs = x\nencoder = keras.Model(encoder_inputs, encoder_outputs)\n\ndecoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\nencoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\nx = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n# x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\nfor _ in range(num_de_transformer_blocks):\n    x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n    x = layers.Dropout(0)(x)\ndecoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\ndecoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n\ndecoder_outputs = decoder([decoder_inputs, encoder_outputs])\ntransformer = keras.Model(\n    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 100  # This should be at least 30 for convergence\n\ncheckpoint = ModelCheckpoint('transformer_model_best_val.h5', #อย่าลืมเปลี่ยน path ให้ file\n                             verbose=1,\n                             monitor='val_accuracy',\n                             save_best_only=True,\n                             mode='max')\n\ntransformer.summary()\ntransformer.compile(\n    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.fit(train_ds, epochs=epochs, validation_data=val_ds,callbacks=[checkpoint])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ans_vocab = ans_vectorization.get_vocabulary()\nans_index_lookup = dict(zip(range(len(ans_vocab)), ans_vocab))\nmax_decoded_sentence_length = 32\n\ntransformer = load_model('transformer_model_best_val.h5')\n\ndef decode_sequence(input_sentence):\n    tokenized_input_sentence = input_vectorization([input_sentence])\n    decoded_sentence = \"[start]\"\n    for i in range(max_decoded_sentence_length):\n        tokenized_target_sentence = ans_vectorization([decoded_sentence])[:, :-1]\n        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n\n        sampled_token_index = np.argmax(predictions[0, i, :])\n        sampled_token = ans_index_lookup[sampled_token_index]\n        decoded_sentence += \" \" + sampled_token\n\n        if sampled_token == \"[end]\":\n            break\n    return decoded_sentence\n\n\ntest_input_texts = [pair[0] for pair in test_pairs]\nfor _ in range(30):\n    input_sentence = random.choice(test_input_texts)\n    print(input_sentence)\n    translated = decode_sequence(input_sentence)\n    print(translated)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer.evaluate(test_ds, verbose=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}